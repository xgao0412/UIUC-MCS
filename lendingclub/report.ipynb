{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X = pd.read_csv('loan_stat542.csv')\n",
    "X['Y'] = np.where(X.loan_status=='Fully Paid',0,1)\n",
    "X.drop('loan_status',axis=1,inplace= True)\n",
    "# X.drop('id',axis=1,inplace=True)\n",
    "X['term'] = X['term'].apply(lambda s: np.int8(s.split()[0]))\n",
    "X.drop('grade', axis=1, inplace=True)\n",
    "X.drop(labels='emp_title', axis=1, inplace=True)\n",
    "X['emp_length'].replace(to_replace='10+ years', value='10 years', inplace=True)\n",
    "X['emp_length'].replace('< 1 year', '0 years', inplace=True)\n",
    "def emp_length_to_int(s):\n",
    "    if pd.isnull(s):\n",
    "        return s\n",
    "    else:\n",
    "        return np.int8(s.split()[0])    \n",
    "X['emp_length'] = X['emp_length'].apply(emp_length_to_int)\n",
    "X['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace=True)\n",
    "X['log_annual_inc'] = X['annual_inc'].apply(lambda x: np.log10(x+1))\n",
    "X.drop('annual_inc', axis=1, inplace=True)\n",
    "X.drop('title', axis=1, inplace=True)\n",
    "X.drop(labels='zip_code', axis=1, inplace=True)\n",
    "X['earliest_cr_line'] = X['earliest_cr_line'].apply(lambda s: int(s[-4:]))\n",
    "X['fico_score'] = 0.5*X['fico_range_low'] + 0.5*X['fico_range_high']\n",
    "X.drop(['fico_range_high', 'fico_range_low'], axis=1, inplace=True)\n",
    "X['log_revol_bal'] = X['revol_bal'].apply(lambda x: np.log10(x+1))\n",
    "X.drop('revol_bal', axis=1, inplace=True)\n",
    "X = pd.get_dummies(X, \n",
    "                   columns=['sub_grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status', 'application_type'], \n",
    "                   drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "#### First, I append the train and test data and create response variable 'Y' by assigning 'default' to 1 and others to 0. I changed the 'term' to the number of months, dropped the 'grade' since it is a subset of 'sub-grade'. 'emp_title' is dropped too as the employment doesn't provide meaningful information. Changed the 'emp_length' to the number of years. Grouped the 'NONE','ANY' into 'OTHER' for 'home_ownership' variable. Took log of 'annual_inc' because the large range of this variable. Also dropped the 'title' as 'purpose' already provide the information. Kept the years of 'earliest_cr_line'. Created a new variable by taking the mean of 'fico_range_high' and 'fico_range_low'. Took log for 'revol_bal' as the data is skewed. Finally, called get_dummies to convert categorial variable to indicator variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split\n",
    "test_id = pd.read_csv('Project3_test_id.csv')\n",
    "train = X[~X.id.isin(test_id.test1)]\n",
    "test = X[X.id.isin(test_id.test1)]\n",
    "y_train = train['Y']\n",
    "y_test = test['Y']\n",
    "X_train = train.drop(columns = ['Y','id'],axis=1)\n",
    "X_test = test.drop(columns = ['Y','id'],axis=1)\n",
    "train1 = X[~X.id.isin(test_id.test2)]\n",
    "test1 = X[X.id.isin(test_id.test2)]\n",
    "y_train1 = train1['Y']\n",
    "y_test1 = test1['Y']\n",
    "X_train1 = train1.drop(columns = ['Y','id'],axis=1)\n",
    "X_test1 = test1.drop(columns = ['Y','id'],axis=1)\n",
    "train2 = X[~X.id.isin(test_id.test3)]\n",
    "test2 = X[X.id.isin(test_id.test3)]\n",
    "y_train2 = train2['Y']\n",
    "y_test2 = test2['Y']\n",
    "X_train2 = train2.drop(columns = ['Y','id'],axis=1)\n",
    "X_test2 = test2.drop(columns = ['Y','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "def model_logreg(train_x,train_y,test_x,test_y):\n",
    "    pipeline_sgdlogreg = Pipeline([\n",
    "        ('imputer', Imputer(copy=False)), # Mean imputation by default\n",
    "        ('scaler', StandardScaler(copy=False)),\n",
    "        ('model', SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=1, warm_start=True))\n",
    "    ])\n",
    "    param_grid_sgdlogreg = {\n",
    "        'model__alpha': [10**1],\n",
    "        'model__penalty': ['l1']\n",
    "    }\n",
    "    grid_sgd= GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)\n",
    "    grid_sgd.fit(train_x, train_y)\n",
    "    y_prob = grid_sgd.predict_proba(test_x)    \n",
    "    return log_loss(test_y, y_prob)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I use SGD logistice regression for the first model. The model penalty 'l1' and 5 folds are used for each train and test split. The total running time for 3 tests are 90 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   39.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   29.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   29.3s finished\n"
     ]
    }
   ],
   "source": [
    "avg = np.zeros((3,3))\n",
    "avg[0,0] = model_logreg(X_train,y_train,X_test,y_test)\n",
    "avg[0,1] = model_logreg(X_train1,y_train1,X_test1,y_test1)\n",
    "avg[0,2] = model_logreg(X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xgao\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def model_lgb(train_x,train_y,test_x,test_y):\n",
    "    pipeline_xgb = Pipeline([\n",
    "    ('imputer', Imputer(copy=False)), # Mean imputation by default\n",
    "    ('scaler', StandardScaler(copy=False)),\n",
    "    ('model', lgb.LGBMClassifier(silent=False,random_state=1))])\n",
    "\n",
    "    param_xgb = {\n",
    "        'model__max_depth': [50],\n",
    "        'model__learning_rate': [0.1],\n",
    "        'model__n_estimators': [200]\n",
    "    }\n",
    "    grid_xgb = GridSearchCV(estimator=pipeline_xgb, param_grid=param_xgb, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=3, verbose=1, return_train_score=False)\n",
    "    grid_xgb.fit(train_x, train_y)\n",
    "    y_prob = grid_xgb.predict_proba(test_x)    \n",
    "    return log_loss(test_y, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the second model, I used lightgbm classifier as it's faster in training compared to xgboost. First I tried using max_depth =50, learning_rate =0.1, n_estimators=200 and this model is enough to beat the the score. The total training time for this model is 4.5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "avg[1,0] = model_lgb(X_train,y_train,X_test,y_test)\n",
    "avg[1,1] = model_lgb(X_train1,y_train1,X_test1,y_test1)\n",
    "avg[1,2] = model_lgb(X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second model, random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def model_rfc(train_x,train_y,test_x,test_y):\n",
    "    pipeline_rfc = Pipeline([\n",
    "        ('imputer', Imputer(copy=False)),\n",
    "        ('model', RandomForestClassifier(n_jobs=-1, random_state=1))\n",
    "    ])\n",
    "    param_grid_rfc = {\n",
    "        'model__n_estimators': [50] # The number of randomized trees to build\n",
    "    }\n",
    "    grid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=3, verbose=1, return_train_score=False)\n",
    "    grid_rfc.fit(train_x,train_y)\n",
    "    y_prob = grid_rfc.predict_proba(test_x)    \n",
    "    return log_loss(test_y, y_prob)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the third model, I used randomforest with n_estimators = 50 as a starting point and it can get an average of 0.48 log loss. Since the training is sluggish so I didn't tried other parameters. The training time is 6.7mins for three splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "avg[2,0] = model_rfc(X_train,y_train,X_test,y_test)\n",
    "avg[2,1] = model_rfc(X_train1,y_train1,X_test1,y_test1)\n",
    "avg[2,2] = model_rfc(X_train2,y_train2,X_test2,y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test1</th>\n",
       "      <th>test2</th>\n",
       "      <th>test3</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.558993</td>\n",
       "      <td>0.653265</td>\n",
       "      <td>0.563753</td>\n",
       "      <td>0.592004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.446692</td>\n",
       "      <td>0.448138</td>\n",
       "      <td>0.447188</td>\n",
       "      <td>0.447340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.479729</td>\n",
       "      <td>0.482980</td>\n",
       "      <td>0.479775</td>\n",
       "      <td>0.480828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           test1     test2     test3   average\n",
       "model1  0.558993  0.653265  0.563753  0.592004\n",
       "model2  0.446692  0.448138  0.447188  0.447340\n",
       "model3  0.479729  0.482980  0.479775  0.480828"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = pd.DataFrame(avg,columns=['test1','test2','test3'],index=['model1','model2','model3'])\n",
    "final_result['average'] = final_result.mean(1)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 using lightgbm shows an average of 0.44734 while logistic regression only gives 0.59 and random forest is 0.48"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
